{"cells":[{"cell_type":"markdown","id":"TNkz8LrFC9qA","metadata":{"id":"TNkz8LrFC9qA"},"source":["# DeepLabCut for your standard (single animal) projects!\n","\n","Some useful links:\n","\n","- [DeepLabCut's GitHub: github.com/DeepLabCut/DeepLabCut](https://github.com/DeepLabCut/DeepLabCut)\n","- [DeepLabCut's Documentation: User Guide for Single Animal projects](https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html)\n","\n","\n","This notebook illustrates how to use the cloud to:\n","- create a training set\n","- train a network\n","- evaluate a network\n","- create simple quality check plots\n","- analyze novel videos!\n","\n","### This notebook assumes you already have a project folder with labeled data!\n","\n","This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n","\n","This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview \u0026 the protocol paper!\n","\n","Nath\\*, Mathis\\* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n","\n","\n","Paper: https://www.nature.com/articles/s41596-019-0176-0\n","\n","Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n"]},{"cell_type":"code","execution_count":1,"id":"fPzM3Drq1Vim","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":51101,"status":"ok","timestamp":1758054284258,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"fPzM3Drq1Vim","outputId":"5b76ff35-7681-4998-fd0c-f0893cf8cbb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/DeepLabCut/DeepLabCut.git\n","  Cloning https://github.com/DeepLabCut/DeepLabCut.git to /tmp/pip-req-build-lvomb2xt\n","  Running command git clone --filter=blob:none --quiet https://github.com/DeepLabCut/DeepLabCut.git /tmp/pip-req-build-lvomb2xt\n","  Resolved https://github.com/DeepLabCut/DeepLabCut.git to commit 5fb2525df9f8b49cf1d270c486fac5c2c66e4357\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting albumentations\u003c=1.4.3 (from deeplabcut==3.0.0rc12)\n","  Downloading albumentations-1.4.3-py3-none-any.whl.metadata (37 kB)\n","Collecting dlclibrary\u003e=0.0.7 (from deeplabcut==3.0.0rc12)\n","  Downloading dlclibrary-0.0.11-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (0.8.1)\n","Collecting filterpy\u003e=1.4.4 (from deeplabcut==3.0.0rc12)\n","  Downloading filterpy-1.4.5.zip (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ruamel.yaml\u003e=0.15.0 (from deeplabcut==3.0.0rc12)\n","  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n","Collecting imgaug\u003e=0.4.0 (from deeplabcut==3.0.0rc12)\n","  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (0.6.0)\n","Requirement already satisfied: numba\u003e=0.54 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (0.60.0)\n","Collecting matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3 (from deeplabcut==3.0.0rc12)\n","  Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","Requirement already satisfied: networkx\u003e=2.6 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (3.5)\n","Collecting numpy\u003c2.0.0,\u003e=1.18.5 (from deeplabcut==3.0.0rc12)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m535.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas!=1.5.0,\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (2.2.2)\n","Requirement already satisfied: scikit-image\u003e=0.17 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (0.25.2)\n","Requirement already satisfied: scikit-learn\u003e=1.0 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (1.6.1)\n","Requirement already satisfied: scipy\u003e=1.9 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (1.16.1)\n","Requirement already satisfied: statsmodels\u003e=0.11 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (0.14.5)\n","Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (1.0.19)\n","Requirement already satisfied: torch\u003e=2.0.0 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (0.23.0+cu126)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (4.67.1)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (2.0.10)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (6.0.2)\n","Requirement already satisfied: Pillow\u003e=7.1 in /usr/local/lib/python3.12/dist-packages (from deeplabcut==3.0.0rc12) (11.3.0)\n","Requirement already satisfied: typing-extensions\u003e=4.9.0 in /usr/local/lib/python3.12/dist-packages (from albumentations\u003c=1.4.3-\u003edeeplabcut==3.0.0rc12) (4.15.0)\n","Requirement already satisfied: opencv-python-headless\u003e=4.9.0 in /usr/local/lib/python3.12/dist-packages (from albumentations\u003c=1.4.3-\u003edeeplabcut==3.0.0rc12) (4.12.0.88)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from dlclibrary\u003e=0.0.7-\u003edeeplabcut==3.0.0rc12) (0.34.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from imgaug\u003e=0.4.0-\u003edeeplabcut==3.0.0rc12) (1.17.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from imgaug\u003e=0.4.0-\u003edeeplabcut==3.0.0rc12) (4.12.0.88)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from imgaug\u003e=0.4.0-\u003edeeplabcut==3.0.0rc12) (2.37.0)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from imgaug\u003e=0.4.0-\u003edeeplabcut==3.0.0rc12) (2.1.1)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3-\u003edeeplabcut==3.0.0rc12) (1.3.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3-\u003edeeplabcut==3.0.0rc12) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3-\u003edeeplabcut==3.0.0rc12) (4.59.2)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3-\u003edeeplabcut==3.0.0rc12) (1.4.9)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3-\u003edeeplabcut==3.0.0rc12) (25.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3-\u003edeeplabcut==3.0.0rc12) (3.2.3)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.7.0,!=3.7.1,\u003c3.9,\u003e=3.3-\u003edeeplabcut==3.0.0rc12) (2.9.0.post0)\n","Requirement already satisfied: llvmlite\u003c0.44,\u003e=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba\u003e=0.54-\u003edeeplabcut==3.0.0rc12) (0.43.0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=1.5.0,\u003e=1.0.1-\u003edeeplabcut==3.0.0rc12) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=1.5.0,\u003e=1.0.1-\u003edeeplabcut==3.0.0rc12) (2025.2)\n","Collecting ruamel.yaml.clib\u003e=0.2.7 (from ruamel.yaml\u003e=0.15.0-\u003edeeplabcut==3.0.0rc12)\n","  Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: tifffile\u003e=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image\u003e=0.17-\u003edeeplabcut==3.0.0rc12) (2025.8.28)\n","Requirement already satisfied: lazy-loader\u003e=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image\u003e=0.17-\u003edeeplabcut==3.0.0rc12) (0.4)\n","Requirement already satisfied: joblib\u003e=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn\u003e=1.0-\u003edeeplabcut==3.0.0rc12) (1.5.2)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn\u003e=1.0-\u003edeeplabcut==3.0.0rc12) (3.6.0)\n","Requirement already satisfied: patsy\u003e=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels\u003e=0.11-\u003edeeplabcut==3.0.0rc12) (1.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (3.19.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (75.2.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (1.13.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (3.4.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm-\u003edeeplabcut==3.0.0rc12) (0.6.2)\n","INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-python-headless\u003e=4.9.0 (from albumentations\u003c=1.4.3-\u003edeeplabcut==3.0.0rc12)\n","  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (1.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub-\u003edlclibrary\u003e=0.0.7-\u003edeeplabcut==3.0.0rc12) (2.32.4)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub-\u003edlclibrary\u003e=0.0.7-\u003edeeplabcut==3.0.0rc12) (1.1.9)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch\u003e=2.0.0-\u003edeeplabcut==3.0.0rc12) (3.0.2)\n","INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-python (from imgaug\u003e=0.4.0-\u003edeeplabcut==3.0.0rc12)\n","  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003edlclibrary\u003e=0.0.7-\u003edeeplabcut==3.0.0rc12) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003edlclibrary\u003e=0.0.7-\u003edeeplabcut==3.0.0rc12) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003edlclibrary\u003e=0.0.7-\u003edeeplabcut==3.0.0rc12) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub-\u003edlclibrary\u003e=0.0.7-\u003edeeplabcut==3.0.0rc12) (2025.8.3)\n","Downloading albumentations-1.4.3-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dlclibrary-0.0.11-py3-none-any.whl (17 kB)\n","Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (754 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: deeplabcut, filterpy\n","  Building wheel for deeplabcut (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deeplabcut: filename=deeplabcut-3.0.0rc12-py3-none-any.whl size=2157643 sha256=532fbd6ba88b89b81bc8575e42a220727e88a7d406cfe809b8d64d48e27b7ade\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9omhcik3/wheels/14/f7/62/c4755de34ca565bed730e9fe98402b580ed7200d9bb0e1121c\n","  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110460 sha256=9095503afce116533c6c76ecc27d244223b0138d9dae42c5e2e49c3bf697c861\n","  Stored in directory: /root/.cache/pip/wheels/77/bf/4c/b0c3f4798a0166668752312a67118b27a3cd341e13ac0ae6ee\n","Successfully built deeplabcut filterpy\n","Installing collected packages: ruamel.yaml.clib, numpy, ruamel.yaml, opencv-python-headless, opencv-python, matplotlib, dlclibrary, imgaug, filterpy, albumentations, deeplabcut\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.12.0.88\n","    Uninstalling opencv-python-headless-4.12.0.88:\n","      Successfully uninstalled opencv-python-headless-4.12.0.88\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.12.0.88\n","    Uninstalling opencv-python-4.12.0.88:\n","      Successfully uninstalled opencv-python-4.12.0.88\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 2.0.8\n","    Uninstalling albumentations-2.0.8:\n","      Successfully uninstalled albumentations-2.0.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-contrib-python 4.12.0.88 requires numpy\u003c2.3.0,\u003e=2; python_version \u003e= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","thinc 8.3.6 requires numpy\u003c3.0.0,\u003e=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed albumentations-1.4.3 deeplabcut-3.0.0rc12 dlclibrary-0.0.11 filterpy-1.4.5 imgaug-0.4.0 matplotlib-3.8.4 numpy-1.26.4 opencv-python-4.11.0.86 opencv-python-headless-4.11.0.86 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.12\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"ec3d8e34aafb45f9ae720fdc1af82425","pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install git+https://github.com/DeepLabCut/DeepLabCut.git\n"]},{"cell_type":"markdown","id":"y-NH9wb1C3P6","metadata":{"id":"y-NH9wb1C3P6"},"source":["**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !)** You will see this button at the output of the cells above ^."]},{"cell_type":"code","execution_count":1,"id":"f054e00d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39027,"status":"ok","timestamp":1758054400927,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"f054e00d","outputId":"ea7ed64b-ae01-40af-c7c3-738eaa5dc887"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading DLC 3.0.0rc12...\n","DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"]}],"source":["# Import necessary libraries\n","import torch\n","import deeplabcut"]},{"cell_type":"markdown","id":"ciAnPwSIDEr8","metadata":{"id":"ciAnPwSIDEr8"},"source":["## Link your Google Drive (with your labeled data, or the demo data):\n","\n","### First, place your project folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into google drive."]},{"cell_type":"code","execution_count":21,"id":"pbhSzxez2iL9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":848,"status":"ok","timestamp":1758057261891,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"pbhSzxez2iL9","outputId":"e9053983-ac8a-4a91-d6b2-a1f7f65e8e7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Now, let's link to your GoogleDrive. Run this cell and follow the authorization instructions:\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"OzdoMYTnDPiP","metadata":{"id":"OzdoMYTnDPiP"},"source":["YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n","\n","Typically, this will be: `/content/drive/My Drive/yourProjectFolderName`\n","or were you choose to place the project folder\n","\n","Tip: Using the folder icon on the left hand panel, you can navigate to where your DLC project folder is located, and copy-paste the path to the config.yaml file directly.\n"]},{"cell_type":"code","execution_count":3,"id":"13ffb91e","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1758054424337,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"13ffb91e"},"outputs":[],"source":["# Define the path to the config file\n","config = \"/content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/config.yaml\""]},{"cell_type":"markdown","id":"j4rP1agiETL-","metadata":{"id":"j4rP1agiETL-"},"source":["## Create a training dataset:\n","\n","### You must do this step inside of Colab\n","\n","After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n","\n","This function also creates new subdirectories under **dlc-models-pytorch** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pytorch_config.yaml**.\n","\n","Now it is the time to start training the network!\n","\n","Tip: Setting the shuffle when creating a test-train dataset split lets us use the same test-train split repeatedly if needed, e.g. if benchmarking different model architectures on the same dataset, allowing a fair comparison of model performance.\n"]},{"cell_type":"code","execution_count":7,"id":"8qKQsI8E12T5","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1758054464320,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"8qKQsI8E12T5"},"outputs":[],"source":["shuffle = 1001"]},{"cell_type":"code","execution_count":null,"id":"LoRsOankGsBO","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":4642,"status":"error","timestamp":1758009457836,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-60},"id":"LoRsOankGsBO","outputId":"5d6babd4-d1eb-41e9-f1d1-407e1b7df6d4"},"outputs":[{"ename":"ValueError","evalue":"Cannot create shuffle 1001 as it already exists - you must either create the dataset with `userfeedback=False` or delete the shuffle with index 1001 manually (in `dlc-models`/`dlc-models-pytorch` and in the `training-datasets` folder) if you want to create a new shuffle with that index. You can otherwise create a shuffle with a new index. Existing indices are [1001].","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2078222737.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 5\u001b[0;31m deeplabcut.create_training_dataset(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnet_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet_50\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py\u001b[0m in \u001b[0;36mcreate_training_dataset\u001b[0;34m(config, num_shuffles, Shuffles, windows2linux, userfeedback, trainIndices, testIndices, net_type, detector_type, augmenter_type, posecfg_template, superanimal_name, weight_init, engine, ctd_conditions)\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauxfun_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlcparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1125\u001b[0;31m         \u001b[0mShuffles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_shuffles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShuffles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shuffles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserfeedback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainIndices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtestIndices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py\u001b[0m in \u001b[0;36mvalidate_shuffles\u001b[0;34m(cfg, shuffles, num_shuffles, userfeedback)\u001b[0m\n\u001b[1;32m   1450\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mshuffle_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshuffles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muserfeedback\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_shuffles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1452\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1453\u001b[0m                     \u001b[0;34mf\"Cannot create shuffle {shuffle_idx} as it already exists - \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m                     \u001b[0;34mf\"you must either create the dataset with `userfeedback=False` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot create shuffle 1001 as it already exists - you must either create the dataset with `userfeedback=False` or delete the shuffle with index 1001 manually (in `dlc-models`/`dlc-models-pytorch` and in the `training-datasets` folder) if you want to create a new shuffle with that index. You can otherwise create a shuffle with a new index. Existing indices are [1001]."]}],"source":["# There are many more functions you can set here, including which network to use!\n","# Check the docstring for `create_training_dataset` for all options you can use!\n","\n","\n","deeplabcut.create_training_dataset(\n","    config,\n","    net_type=\"resnet_50\",\n","    engine=deeplabcut.Engine.PYTORCH,\n","    Shuffles=[shuffle]  #specify which shuffle index you want - if left empty it will use the next available shuffle\n",")"]},{"cell_type":"code","execution_count":null,"id":"e705b31d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1790168,"status":"error","timestamp":1758011253054,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-60},"id":"e705b31d","outputId":"bf1367bb-36af-4d12-d63a-b92fc07a7162"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training with configuration:\n","data:\n","  bbox_margin: 20\n","  colormode: RGB\n","  inference:\n","    normalize_images: True\n","  train:\n","    affine:\n","      p: 0.5\n","      rotation: 30\n","      scaling: [0.5, 1.25]\n","      translation: 0\n","    crop_sampling:\n","      width: 448\n","      height: 448\n","      max_shift: 0.1\n","      method: hybrid\n","    gaussian_noise: 12.75\n","    motion_blur: True\n","    normalize_images: True\n","device: auto\n","metadata:\n","  project_path: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run\n","  pose_config_path: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/dlc-models-pytorch/iteration-0/openfieldOct30-trainset95shuffle1001/train/pytorch_config.yaml\n","  bodyparts: ['snout', 'leftear', 'rightear', 'tailbase']\n","  unique_bodyparts: []\n","  individuals: ['animal']\n","  with_identity: None\n","method: bu\n","model:\n","  backbone:\n","    type: ResNet\n","    model_name: resnet50_gn\n","    output_stride: 16\n","    freeze_bn_stats: False\n","    freeze_bn_weights: False\n","  backbone_output_channels: 2048\n","  heads:\n","    bodypart:\n","      type: HeatmapHead\n","      weight_init: normal\n","      predictor:\n","        type: HeatmapPredictor\n","        apply_sigmoid: False\n","        clip_scores: True\n","        location_refinement: True\n","        locref_std: 7.2801\n","      target_generator:\n","        type: HeatmapGaussianGenerator\n","        num_heatmaps: 4\n","        pos_dist_thresh: 17\n","        heatmap_mode: KEYPOINT\n","        gradient_masking: False\n","        generate_locref: True\n","        locref_std: 7.2801\n","      criterion:\n","        heatmap:\n","          type: WeightedMSECriterion\n","          weight: 1.0\n","        locref:\n","          type: WeightedHuberCriterion\n","          weight: 0.05\n","      heatmap_config:\n","        channels: [2048, 4]\n","        kernel_size: [3]\n","        strides: [2]\n","      locref_config:\n","        channels: [2048, 8]\n","        kernel_size: [3]\n","        strides: [2]\n","net_type: resnet_50\n","runner:\n","  type: PoseTrainingRunner\n","  gpus: None\n","  key_metric: test.mAP\n","  key_metric_asc: True\n","  eval_interval: 10\n","  optimizer:\n","    type: AdamW\n","    params:\n","      lr: 0.0005\n","  scheduler:\n","    type: LRListScheduler\n","    params:\n","      lr_list: [[0.0001], [1e-05]]\n","      milestones: [90, 120]\n","  snapshots:\n","    max_snapshots: 5\n","    save_epochs: 5\n","    save_optimizer_state: False\n","train_settings:\n","  batch_size: 8\n","  dataloader_workers: 0\n","  dataloader_pin_memory: False\n","  display_iters: 500\n","  epochs: 200\n","  seed: 42\n","Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5d23d35dbb740a08ad26d6126499bca","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/102M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n","Data Transforms:\n","  Training:   Compose([\n","  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),\n","  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),\n","  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),\n","  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),\n","  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),\n","  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n","], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n","  Validation: Compose([\n","  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n","], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n","Using 110 images and 6 for testing\n","\n","Starting pose model training...\n","--------------------------------------------------\n","Epoch 1/200 (lr=0.0005), train loss 0.01531\n","Epoch 2/200 (lr=0.0005), train loss 0.01184\n","Epoch 3/200 (lr=0.0005), train loss 0.00695\n","Epoch 4/200 (lr=0.0005), train loss 0.00522\n","Epoch 5/200 (lr=0.0005), train loss 0.00380\n","Epoch 6/200 (lr=0.0005), train loss 0.00272\n","Epoch 7/200 (lr=0.0005), train loss 0.00211\n","Epoch 8/200 (lr=0.0005), train loss 0.00188\n","Epoch 9/200 (lr=0.0005), train loss 0.00224\n","Training for epoch 10 done, starting evaluation\n","Epoch 10/200 (lr=0.0005), train loss 0.00217, valid loss 0.00165\n","Model performance:\n","  metrics/test.rmse:           3.14\n","  metrics/test.rmse_pcutoff:   3.19\n","  metrics/test.mAP:           95.64\n","  metrics/test.mAR:           96.67\n","Epoch 11/200 (lr=0.0005), train loss 0.00198\n","Epoch 12/200 (lr=0.0005), train loss 0.00161\n","Epoch 13/200 (lr=0.0005), train loss 0.00160\n","Epoch 14/200 (lr=0.0005), train loss 0.00172\n","Epoch 15/200 (lr=0.0005), train loss 0.00146\n","Epoch 16/200 (lr=0.0005), train loss 0.00133\n","Epoch 17/200 (lr=0.0005), train loss 0.00146\n","Epoch 18/200 (lr=0.0005), train loss 0.00132\n","Epoch 19/200 (lr=0.0005), train loss 0.00188\n","Training for epoch 20 done, starting evaluation\n","Epoch 20/200 (lr=0.0005), train loss 0.00182, valid loss 0.00154\n","Model performance:\n","  metrics/test.rmse:           3.20\n","  metrics/test.rmse_pcutoff:   2.87\n","  metrics/test.mAP:           96.63\n","  metrics/test.mAR:           96.67\n","Epoch 21/200 (lr=0.0005), train loss 0.00142\n","Epoch 22/200 (lr=0.0005), train loss 0.00122\n","Epoch 23/200 (lr=0.0005), train loss 0.00144\n","Epoch 24/200 (lr=0.0005), train loss 0.00127\n","Epoch 25/200 (lr=0.0005), train loss 0.00104\n","Epoch 26/200 (lr=0.0005), train loss 0.00113\n","Epoch 27/200 (lr=0.0005), train loss 0.00112\n","Epoch 28/200 (lr=0.0005), train loss 0.00102\n","Epoch 29/200 (lr=0.0005), train loss 0.00115\n","Training for epoch 30 done, starting evaluation\n","Epoch 30/200 (lr=0.0005), train loss 0.00106, valid loss 0.00105\n","Model performance:\n","  metrics/test.rmse:           2.70\n","  metrics/test.rmse_pcutoff:   2.70\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 31/200 (lr=0.0005), train loss 0.00117\n","Epoch 32/200 (lr=0.0005), train loss 0.00102\n","Epoch 33/200 (lr=0.0005), train loss 0.00110\n","Epoch 34/200 (lr=0.0005), train loss 0.00111\n","Epoch 35/200 (lr=0.0005), train loss 0.00095\n","Epoch 36/200 (lr=0.0005), train loss 0.00099\n","Epoch 37/200 (lr=0.0005), train loss 0.00098\n","Epoch 38/200 (lr=0.0005), train loss 0.00092\n","Epoch 39/200 (lr=0.0005), train loss 0.00104\n","Training for epoch 40 done, starting evaluation\n","Epoch 40/200 (lr=0.0005), train loss 0.00117, valid loss 0.00075\n","Model performance:\n","  metrics/test.rmse:           2.17\n","  metrics/test.rmse_pcutoff:   2.17\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 41/200 (lr=0.0005), train loss 0.00133\n","Epoch 42/200 (lr=0.0005), train loss 0.00103\n","Epoch 43/200 (lr=0.0005), train loss 0.00112\n","Epoch 44/200 (lr=0.0005), train loss 0.00099\n","Epoch 45/200 (lr=0.0005), train loss 0.00095\n","Epoch 46/200 (lr=0.0005), train loss 0.00115\n","Epoch 47/200 (lr=0.0005), train loss 0.00095\n","Epoch 48/200 (lr=0.0005), train loss 0.00121\n","Epoch 49/200 (lr=0.0005), train loss 0.00111\n","Training for epoch 50 done, starting evaluation\n","Epoch 50/200 (lr=0.0005), train loss 0.00081, valid loss 0.00086\n","Model performance:\n","  metrics/test.rmse:           2.23\n","  metrics/test.rmse_pcutoff:   2.23\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 51/200 (lr=0.0005), train loss 0.00090\n","Epoch 52/200 (lr=0.0005), train loss 0.00099\n","Epoch 53/200 (lr=0.0005), train loss 0.00082\n","Epoch 54/200 (lr=0.0005), train loss 0.00091\n","Epoch 55/200 (lr=0.0005), train loss 0.00100\n","Epoch 56/200 (lr=0.0005), train loss 0.00079\n","Epoch 57/200 (lr=0.0005), train loss 0.00085\n","Epoch 58/200 (lr=0.0005), train loss 0.00088\n","Epoch 59/200 (lr=0.0005), train loss 0.00098\n","Training for epoch 60 done, starting evaluation\n","Epoch 60/200 (lr=0.0005), train loss 0.00077, valid loss 0.00069\n","Model performance:\n","  metrics/test.rmse:           1.88\n","  metrics/test.rmse_pcutoff:   1.88\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 61/200 (lr=0.0005), train loss 0.00082\n","Epoch 62/200 (lr=0.0005), train loss 0.00077\n","Epoch 63/200 (lr=0.0005), train loss 0.00082\n","Epoch 64/200 (lr=0.0005), train loss 0.00079\n","Epoch 65/200 (lr=0.0005), train loss 0.00077\n","Epoch 66/200 (lr=0.0005), train loss 0.00077\n","Epoch 67/200 (lr=0.0005), train loss 0.00083\n","Epoch 68/200 (lr=0.0005), train loss 0.00091\n","Epoch 69/200 (lr=0.0005), train loss 0.00073\n","Training for epoch 70 done, starting evaluation\n","Epoch 70/200 (lr=0.0005), train loss 0.00090, valid loss 0.00053\n","Model performance:\n","  metrics/test.rmse:           1.69\n","  metrics/test.rmse_pcutoff:   1.69\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 71/200 (lr=0.0005), train loss 0.00082\n","Epoch 72/200 (lr=0.0005), train loss 0.00072\n","Epoch 73/200 (lr=0.0005), train loss 0.00070\n","Epoch 74/200 (lr=0.0005), train loss 0.00074\n","Epoch 75/200 (lr=0.0005), train loss 0.00073\n","Epoch 76/200 (lr=0.0005), train loss 0.00072\n","Epoch 77/200 (lr=0.0005), train loss 0.00070\n","Epoch 78/200 (lr=0.0005), train loss 0.00067\n","Epoch 79/200 (lr=0.0005), train loss 0.00087\n","Training for epoch 80 done, starting evaluation\n","Epoch 80/200 (lr=0.0005), train loss 0.00079, valid loss 0.00062\n","Model performance:\n","  metrics/test.rmse:           1.79\n","  metrics/test.rmse_pcutoff:   1.79\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 81/200 (lr=0.0005), train loss 0.00076\n","Epoch 82/200 (lr=0.0005), train loss 0.00083\n","Epoch 83/200 (lr=0.0005), train loss 0.00087\n","Epoch 84/200 (lr=0.0005), train loss 0.00077\n","Epoch 85/200 (lr=0.0005), train loss 0.00069\n","Epoch 86/200 (lr=0.0005), train loss 0.00071\n","Epoch 87/200 (lr=0.0005), train loss 0.00070\n","Epoch 88/200 (lr=0.0005), train loss 0.00074\n","Epoch 89/200 (lr=0.0005), train loss 0.00081\n","Training for epoch 90 done, starting evaluation\n","Epoch 90/200 (lr=0.0001), train loss 0.00061, valid loss 0.00060\n","Model performance:\n","  metrics/test.rmse:           1.78\n","  metrics/test.rmse_pcutoff:   1.78\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 91/200 (lr=0.0001), train loss 0.00066\n","Epoch 92/200 (lr=0.0001), train loss 0.00061\n","Epoch 93/200 (lr=0.0001), train loss 0.00049\n","Epoch 94/200 (lr=0.0001), train loss 0.00053\n","Epoch 95/200 (lr=0.0001), train loss 0.00052\n","Epoch 96/200 (lr=0.0001), train loss 0.00048\n","Epoch 97/200 (lr=0.0001), train loss 0.00055\n","Epoch 98/200 (lr=0.0001), train loss 0.00051\n","Epoch 99/200 (lr=0.0001), train loss 0.00045\n","Training for epoch 100 done, starting evaluation\n","Epoch 100/200 (lr=0.0001), train loss 0.00048, valid loss 0.00029\n","Model performance:\n","  metrics/test.rmse:           1.18\n","  metrics/test.rmse_pcutoff:   1.18\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 101/200 (lr=0.0001), train loss 0.00053\n","Epoch 102/200 (lr=0.0001), train loss 0.00052\n","Epoch 103/200 (lr=0.0001), train loss 0.00040\n","Epoch 104/200 (lr=0.0001), train loss 0.00046\n","Epoch 105/200 (lr=0.0001), train loss 0.00042\n","Epoch 106/200 (lr=0.0001), train loss 0.00051\n","Epoch 107/200 (lr=0.0001), train loss 0.00056\n","Epoch 108/200 (lr=0.0001), train loss 0.00053\n","Epoch 109/200 (lr=0.0001), train loss 0.00047\n","Training for epoch 110 done, starting evaluation\n","Epoch 110/200 (lr=0.0001), train loss 0.00042, valid loss 0.00029\n","Model performance:\n","  metrics/test.rmse:           1.21\n","  metrics/test.rmse_pcutoff:   1.21\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 111/200 (lr=0.0001), train loss 0.00046\n","Epoch 112/200 (lr=0.0001), train loss 0.00049\n","Epoch 113/200 (lr=0.0001), train loss 0.00043\n","Epoch 114/200 (lr=0.0001), train loss 0.00047\n","Epoch 115/200 (lr=0.0001), train loss 0.00041\n","Epoch 116/200 (lr=0.0001), train loss 0.00051\n","Epoch 117/200 (lr=0.0001), train loss 0.00047\n","Epoch 118/200 (lr=0.0001), train loss 0.00043\n","Epoch 119/200 (lr=0.0001), train loss 0.00043\n","Training for epoch 120 done, starting evaluation\n","Epoch 120/200 (lr=1e-05), train loss 0.00058, valid loss 0.00025\n","Model performance:\n","  metrics/test.rmse:           1.18\n","  metrics/test.rmse_pcutoff:   1.18\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 121/200 (lr=1e-05), train loss 0.00055\n","Epoch 122/200 (lr=1e-05), train loss 0.00041\n","Epoch 123/200 (lr=1e-05), train loss 0.00047\n","Epoch 124/200 (lr=1e-05), train loss 0.00043\n","Epoch 125/200 (lr=1e-05), train loss 0.00043\n","Epoch 126/200 (lr=1e-05), train loss 0.00046\n","Epoch 127/200 (lr=1e-05), train loss 0.00036\n","Epoch 128/200 (lr=1e-05), train loss 0.00047\n","Epoch 129/200 (lr=1e-05), train loss 0.00046\n","Training for epoch 130 done, starting evaluation\n","Epoch 130/200 (lr=1e-05), train loss 0.00040, valid loss 0.00024\n","Model performance:\n","  metrics/test.rmse:           1.10\n","  metrics/test.rmse_pcutoff:   1.10\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 131/200 (lr=1e-05), train loss 0.00049\n","Epoch 132/200 (lr=1e-05), train loss 0.00038\n","Epoch 133/200 (lr=1e-05), train loss 0.00033\n","Epoch 134/200 (lr=1e-05), train loss 0.00046\n","Epoch 135/200 (lr=1e-05), train loss 0.00042\n","Epoch 136/200 (lr=1e-05), train loss 0.00044\n","Epoch 137/200 (lr=1e-05), train loss 0.00048\n","Epoch 138/200 (lr=1e-05), train loss 0.00048\n","Epoch 139/200 (lr=1e-05), train loss 0.00046\n","Training for epoch 140 done, starting evaluation\n","Epoch 140/200 (lr=1e-05), train loss 0.00044, valid loss 0.00025\n","Model performance:\n","  metrics/test.rmse:           1.18\n","  metrics/test.rmse_pcutoff:   1.18\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 141/200 (lr=1e-05), train loss 0.00042\n","Epoch 142/200 (lr=1e-05), train loss 0.00048\n","Epoch 143/200 (lr=1e-05), train loss 0.00044\n","Epoch 144/200 (lr=1e-05), train loss 0.00044\n","Epoch 145/200 (lr=1e-05), train loss 0.00037\n","Epoch 146/200 (lr=1e-05), train loss 0.00042\n","Epoch 147/200 (lr=1e-05), train loss 0.00037\n","Epoch 148/200 (lr=1e-05), train loss 0.00043\n","Epoch 149/200 (lr=1e-05), train loss 0.00041\n","Training for epoch 150 done, starting evaluation\n","Epoch 150/200 (lr=1e-05), train loss 0.00043, valid loss 0.00024\n","Model performance:\n","  metrics/test.rmse:           1.18\n","  metrics/test.rmse_pcutoff:   1.18\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3288844463.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# utilize the GPU's capabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 11\u001b[0;31m deeplabcut.train_network(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/compat.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, epochs, save_epochs, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix, superanimal_name, superanimal_transfer_learning, engine, device, snapshot_path, detector_path, batch_size, detector_batch_size, detector_epochs, detector_save_epochs, pose_threshold, pytorch_cfg_updates)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_estimation_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 287\u001b[0;31m         return train_network(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/apis/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, modelprefix, device, snapshot_path, detector_path, load_head_weights, batch_size, epochs, save_epochs, detector_batch_size, detector_epochs, detector_save_epochs, display_iters, max_snapshots_to_keep, pose_threshold, pytorch_cfg_updates)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_settings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 374\u001b[0;31m         train(\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mrun_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/apis/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, run_config, task, device, gpus, logger_config, snapshot_path, transform, inference_transform, max_snapshots_to_keep, load_head_weights)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting pose model training...\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 198\u001b[0;31m     runner.fit(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/runners/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, epochs, display_iters)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 214\u001b[0;31m             train_loss = self._epoch(\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/runners/train.py\u001b[0m in \u001b[0;36m_epoch\u001b[0;34m(self, loader, mode, display_iters)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mloss_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 276\u001b[0;31m             \u001b[0mlosses_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"total_loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/runners/train.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_epoch_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train the model using the specified config file\n","\n","# Let's also change the display and save_epochs just in case Colab takes away\n","# the GPU... If that happens, you can reload from a saved point using the\n","# `snapshot_path` argument to `deeplabcut.train_network`:\n","#   deeplabcut.train_network(..., snapshot_path=\"/content/.../snapshot-050.pt\")\n","\n","# Typically, you want to train to ~200 epochs. We set the batch size to 8 to\n","# utilize the GPU's capabilities.\n","\n","deeplabcut.train_network(\n","    config,\n","    shuffle=shuffle,\n","    save_epochs=5, #saves an instance of your model every 5 epochs\n","    epochs=200, #tells you how many epochs (rounds of viewing the whole train dataset) the training session will run for\n","    batch_size=8,\n",")\n","\n","\n","# This will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end, or when the COLAB session ends, whichever comes first."]},{"cell_type":"code","execution_count":7,"id":"_s4vNfQfxF0v","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1883234,"status":"error","timestamp":1758050139252,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"_s4vNfQfxF0v","outputId":"aa323106-ca2c-4582-c273-520fce25ed7c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training with configuration:\n","data:\n","  bbox_margin: 20\n","  colormode: RGB\n","  inference:\n","    normalize_images: True\n","  train:\n","    affine:\n","      p: 0.5\n","      rotation: 30\n","      scaling: [0.5, 1.25]\n","      translation: 0\n","    crop_sampling:\n","      width: 448\n","      height: 448\n","      max_shift: 0.1\n","      method: hybrid\n","    gaussian_noise: 12.75\n","    motion_blur: True\n","    normalize_images: True\n","device: auto\n","metadata:\n","  project_path: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run\n","  pose_config_path: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/dlc-models-pytorch/iteration-0/openfieldOct30-trainset95shuffle1001/train/pytorch_config.yaml\n","  bodyparts: ['snout', 'leftear', 'rightear', 'tailbase']\n","  unique_bodyparts: []\n","  individuals: ['animal']\n","  with_identity: None\n","method: bu\n","model:\n","  backbone:\n","    type: ResNet\n","    model_name: resnet50_gn\n","    output_stride: 16\n","    freeze_bn_stats: False\n","    freeze_bn_weights: False\n","  backbone_output_channels: 2048\n","  heads:\n","    bodypart:\n","      type: HeatmapHead\n","      weight_init: normal\n","      predictor:\n","        type: HeatmapPredictor\n","        apply_sigmoid: False\n","        clip_scores: True\n","        location_refinement: True\n","        locref_std: 7.2801\n","      target_generator:\n","        type: HeatmapGaussianGenerator\n","        num_heatmaps: 4\n","        pos_dist_thresh: 17\n","        heatmap_mode: KEYPOINT\n","        gradient_masking: False\n","        generate_locref: True\n","        locref_std: 7.2801\n","      criterion:\n","        heatmap:\n","          type: WeightedMSECriterion\n","          weight: 1.0\n","        locref:\n","          type: WeightedHuberCriterion\n","          weight: 0.05\n","      heatmap_config:\n","        channels: [2048, 4]\n","        kernel_size: [3]\n","        strides: [2]\n","      locref_config:\n","        channels: [2048, 8]\n","        kernel_size: [3]\n","        strides: [2]\n","net_type: resnet_50\n","runner:\n","  type: PoseTrainingRunner\n","  gpus: None\n","  key_metric: test.mAP\n","  key_metric_asc: True\n","  eval_interval: 10\n","  optimizer:\n","    type: AdamW\n","    params:\n","      lr: 0.0005\n","  scheduler:\n","    type: LRListScheduler\n","    params:\n","      lr_list: [[0.0001], [1e-05]]\n","      milestones: [90, 120]\n","  snapshots:\n","    max_snapshots: 5\n","    save_epochs: 5\n","    save_optimizer_state: False\n","train_settings:\n","  batch_size: 8\n","  dataloader_workers: 0\n","  dataloader_pin_memory: False\n","  display_iters: 500\n","  epochs: 200\n","  seed: 42\n","Data Transforms:\n","  Training:   Compose([\n","  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),\n","  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),\n","  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),\n","  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),\n","  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),\n","  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n","], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n","  Validation: Compose([\n","  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n","], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n","Using 110 images and 6 for testing\n","\n","Starting pose model training...\n","--------------------------------------------------\n","Epoch 151/350 (lr=1e-05), train loss 0.00037\n","Epoch 152/350 (lr=1e-05), train loss 0.00046\n","Epoch 153/350 (lr=1e-05), train loss 0.00042\n","Epoch 154/350 (lr=1e-05), train loss 0.00041\n","Epoch 155/350 (lr=1e-05), train loss 0.00045\n","Epoch 156/350 (lr=1e-05), train loss 0.00036\n","Epoch 157/350 (lr=1e-05), train loss 0.00036\n","Epoch 158/350 (lr=1e-05), train loss 0.00035\n","Epoch 159/350 (lr=1e-05), train loss 0.00046\n","Training for epoch 160 done, starting evaluation\n","Epoch 160/350 (lr=1e-05), train loss 0.00037, valid loss 0.00024\n","Model performance:\n","  metrics/test.rmse:           1.20\n","  metrics/test.rmse_pcutoff:   1.20\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 161/350 (lr=1e-05), train loss 0.00036\n","Epoch 162/350 (lr=1e-05), train loss 0.00038\n","Epoch 163/350 (lr=1e-05), train loss 0.00040\n","Epoch 164/350 (lr=1e-05), train loss 0.00039\n","Epoch 165/350 (lr=1e-05), train loss 0.00038\n","Epoch 166/350 (lr=1e-05), train loss 0.00036\n","Epoch 167/350 (lr=1e-05), train loss 0.00043\n","Epoch 168/350 (lr=1e-05), train loss 0.00033\n","Epoch 169/350 (lr=1e-05), train loss 0.00044\n","Training for epoch 170 done, starting evaluation\n","Epoch 170/350 (lr=1e-05), train loss 0.00038, valid loss 0.00024\n","Model performance:\n","  metrics/test.rmse:           1.19\n","  metrics/test.rmse_pcutoff:   1.19\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 171/350 (lr=1e-05), train loss 0.00040\n","Epoch 172/350 (lr=1e-05), train loss 0.00038\n","Epoch 173/350 (lr=1e-05), train loss 0.00046\n","Epoch 174/350 (lr=1e-05), train loss 0.00041\n","Epoch 175/350 (lr=1e-05), train loss 0.00035\n","Epoch 176/350 (lr=1e-05), train loss 0.00039\n","Epoch 177/350 (lr=1e-05), train loss 0.00037\n","Epoch 178/350 (lr=1e-05), train loss 0.00040\n","Epoch 179/350 (lr=1e-05), train loss 0.00039\n","Training for epoch 180 done, starting evaluation\n","Epoch 180/350 (lr=1e-05), train loss 0.00041, valid loss 0.00024\n","Model performance:\n","  metrics/test.rmse:           1.20\n","  metrics/test.rmse_pcutoff:   1.20\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 181/350 (lr=1e-05), train loss 0.00041\n","Epoch 182/350 (lr=1e-05), train loss 0.00037\n","Epoch 183/350 (lr=1e-05), train loss 0.00036\n","Epoch 184/350 (lr=1e-05), train loss 0.00042\n","Epoch 185/350 (lr=1e-05), train loss 0.00038\n","Epoch 186/350 (lr=1e-05), train loss 0.00036\n","Epoch 187/350 (lr=1e-05), train loss 0.00037\n","Epoch 188/350 (lr=1e-05), train loss 0.00038\n","Epoch 189/350 (lr=1e-05), train loss 0.00037\n","Training for epoch 190 done, starting evaluation\n","Epoch 190/350 (lr=1e-05), train loss 0.00040, valid loss 0.00025\n","Model performance:\n","  metrics/test.rmse:           1.25\n","  metrics/test.rmse_pcutoff:   1.25\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 191/350 (lr=1e-05), train loss 0.00040\n","Epoch 192/350 (lr=1e-05), train loss 0.00026\n","Epoch 193/350 (lr=1e-05), train loss 0.00041\n","Epoch 194/350 (lr=1e-05), train loss 0.00037\n","Epoch 195/350 (lr=1e-05), train loss 0.00034\n","Epoch 196/350 (lr=1e-05), train loss 0.00049\n","Epoch 197/350 (lr=1e-05), train loss 0.00038\n","Epoch 198/350 (lr=1e-05), train loss 0.00043\n","Epoch 199/350 (lr=1e-05), train loss 0.00046\n","Training for epoch 200 done, starting evaluation\n","Epoch 200/350 (lr=1e-05), train loss 0.00029, valid loss 0.00025\n","Model performance:\n","  metrics/test.rmse:           1.20\n","  metrics/test.rmse_pcutoff:   1.20\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 201/350 (lr=1e-05), train loss 0.00035\n","Epoch 202/350 (lr=1e-05), train loss 0.00045\n","Epoch 203/350 (lr=1e-05), train loss 0.00034\n","Epoch 204/350 (lr=1e-05), train loss 0.00034\n","Epoch 205/350 (lr=1e-05), train loss 0.00042\n","Epoch 206/350 (lr=1e-05), train loss 0.00034\n","Epoch 207/350 (lr=1e-05), train loss 0.00036\n","Epoch 208/350 (lr=1e-05), train loss 0.00043\n","Epoch 209/350 (lr=1e-05), train loss 0.00032\n","Training for epoch 210 done, starting evaluation\n","Epoch 210/350 (lr=1e-05), train loss 0.00032, valid loss 0.00024\n","Model performance:\n","  metrics/test.rmse:           1.25\n","  metrics/test.rmse_pcutoff:   1.25\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 211/350 (lr=1e-05), train loss 0.00032\n","Epoch 212/350 (lr=1e-05), train loss 0.00038\n","Epoch 213/350 (lr=1e-05), train loss 0.00031\n","Epoch 214/350 (lr=1e-05), train loss 0.00031\n","Epoch 215/350 (lr=1e-05), train loss 0.00035\n","Epoch 216/350 (lr=1e-05), train loss 0.00033\n","Epoch 217/350 (lr=1e-05), train loss 0.00036\n","Epoch 218/350 (lr=1e-05), train loss 0.00036\n","Epoch 219/350 (lr=1e-05), train loss 0.00035\n","Training for epoch 220 done, starting evaluation\n","Epoch 220/350 (lr=1e-05), train loss 0.00040, valid loss 0.00025\n","Model performance:\n","  metrics/test.rmse:           1.26\n","  metrics/test.rmse_pcutoff:   1.26\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 221/350 (lr=1e-05), train loss 0.00042\n","Epoch 222/350 (lr=1e-05), train loss 0.00035\n","Epoch 223/350 (lr=1e-05), train loss 0.00031\n","Epoch 224/350 (lr=1e-05), train loss 0.00034\n","Epoch 225/350 (lr=1e-05), train loss 0.00028\n","Epoch 226/350 (lr=1e-05), train loss 0.00036\n","Epoch 227/350 (lr=1e-05), train loss 0.00036\n","Epoch 228/350 (lr=1e-05), train loss 0.00034\n","Epoch 229/350 (lr=1e-05), train loss 0.00036\n","Training for epoch 230 done, starting evaluation\n","Epoch 230/350 (lr=1e-05), train loss 0.00033, valid loss 0.00023\n","Model performance:\n","  metrics/test.rmse:           1.19\n","  metrics/test.rmse_pcutoff:   1.19\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 231/350 (lr=1e-05), train loss 0.00032\n","Epoch 232/350 (lr=1e-05), train loss 0.00032\n","Epoch 233/350 (lr=1e-05), train loss 0.00038\n","Epoch 234/350 (lr=1e-05), train loss 0.00032\n","Epoch 235/350 (lr=1e-05), train loss 0.00032\n","Epoch 236/350 (lr=1e-05), train loss 0.00028\n","Epoch 237/350 (lr=1e-05), train loss 0.00030\n","Epoch 238/350 (lr=1e-05), train loss 0.00033\n","Epoch 239/350 (lr=1e-05), train loss 0.00032\n","Training for epoch 240 done, starting evaluation\n","Epoch 240/350 (lr=1e-05), train loss 0.00026, valid loss 0.00023\n","Model performance:\n","  metrics/test.rmse:           1.15\n","  metrics/test.rmse_pcutoff:   1.15\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 241/350 (lr=1e-05), train loss 0.00037\n","Epoch 242/350 (lr=1e-05), train loss 0.00039\n","Epoch 243/350 (lr=1e-05), train loss 0.00033\n","Epoch 244/350 (lr=1e-05), train loss 0.00034\n","Epoch 245/350 (lr=1e-05), train loss 0.00034\n","Epoch 246/350 (lr=1e-05), train loss 0.00030\n","Epoch 247/350 (lr=1e-05), train loss 0.00037\n","Epoch 248/350 (lr=1e-05), train loss 0.00036\n","Epoch 249/350 (lr=1e-05), train loss 0.00032\n","Training for epoch 250 done, starting evaluation\n","Epoch 250/350 (lr=1e-05), train loss 0.00033, valid loss 0.00026\n","Model performance:\n","  metrics/test.rmse:           1.27\n","  metrics/test.rmse_pcutoff:   1.27\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 251/350 (lr=1e-05), train loss 0.00039\n","Epoch 252/350 (lr=1e-05), train loss 0.00035\n","Epoch 253/350 (lr=1e-05), train loss 0.00029\n","Epoch 254/350 (lr=1e-05), train loss 0.00036\n","Epoch 255/350 (lr=1e-05), train loss 0.00030\n","Epoch 256/350 (lr=1e-05), train loss 0.00037\n","Epoch 257/350 (lr=1e-05), train loss 0.00040\n","Epoch 258/350 (lr=1e-05), train loss 0.00039\n","Epoch 259/350 (lr=1e-05), train loss 0.00036\n","Training for epoch 260 done, starting evaluation\n","Epoch 260/350 (lr=1e-05), train loss 0.00029, valid loss 0.00021\n","Model performance:\n","  metrics/test.rmse:           1.17\n","  metrics/test.rmse_pcutoff:   1.17\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 261/350 (lr=1e-05), train loss 0.00034\n","Epoch 262/350 (lr=1e-05), train loss 0.00030\n","Epoch 263/350 (lr=1e-05), train loss 0.00033\n","Epoch 264/350 (lr=1e-05), train loss 0.00034\n","Epoch 265/350 (lr=1e-05), train loss 0.00031\n","Epoch 266/350 (lr=1e-05), train loss 0.00034\n","Epoch 267/350 (lr=1e-05), train loss 0.00036\n","Epoch 268/350 (lr=1e-05), train loss 0.00031\n","Epoch 269/350 (lr=1e-05), train loss 0.00033\n","Training for epoch 270 done, starting evaluation\n","Epoch 270/350 (lr=1e-05), train loss 0.00044, valid loss 0.00023\n","Model performance:\n","  metrics/test.rmse:           1.22\n","  metrics/test.rmse_pcutoff:   1.22\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 271/350 (lr=1e-05), train loss 0.00047\n","Epoch 272/350 (lr=1e-05), train loss 0.00036\n","Epoch 273/350 (lr=1e-05), train loss 0.00040\n","Epoch 274/350 (lr=1e-05), train loss 0.00038\n","Epoch 275/350 (lr=1e-05), train loss 0.00037\n","Epoch 276/350 (lr=1e-05), train loss 0.00040\n","Epoch 277/350 (lr=1e-05), train loss 0.00033\n","Epoch 278/350 (lr=1e-05), train loss 0.00042\n","Epoch 279/350 (lr=1e-05), train loss 0.00040\n","Training for epoch 280 done, starting evaluation\n","Epoch 280/350 (lr=1e-05), train loss 0.00036, valid loss 0.00020\n","Model performance:\n","  metrics/test.rmse:           1.16\n","  metrics/test.rmse_pcutoff:   1.16\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 281/350 (lr=1e-05), train loss 0.00045\n","Epoch 282/350 (lr=1e-05), train loss 0.00035\n","Epoch 283/350 (lr=1e-05), train loss 0.00030\n","Epoch 284/350 (lr=1e-05), train loss 0.00042\n","Epoch 285/350 (lr=1e-05), train loss 0.00037\n","Epoch 286/350 (lr=1e-05), train loss 0.00040\n","Epoch 287/350 (lr=1e-05), train loss 0.00044\n","Epoch 288/350 (lr=1e-05), train loss 0.00044\n","Epoch 289/350 (lr=1e-05), train loss 0.00042\n","Training for epoch 290 done, starting evaluation\n","Epoch 290/350 (lr=1e-05), train loss 0.00039, valid loss 0.00025\n","Model performance:\n","  metrics/test.rmse:           1.30\n","  metrics/test.rmse_pcutoff:   1.30\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 291/350 (lr=1e-05), train loss 0.00037\n","Epoch 292/350 (lr=1e-05), train loss 0.00044\n","Epoch 293/350 (lr=1e-05), train loss 0.00038\n","Epoch 294/350 (lr=1e-05), train loss 0.00040\n","Epoch 295/350 (lr=1e-05), train loss 0.00033\n","Epoch 296/350 (lr=1e-05), train loss 0.00038\n","Epoch 297/350 (lr=1e-05), train loss 0.00032\n","Epoch 298/350 (lr=1e-05), train loss 0.00039\n","Epoch 299/350 (lr=1e-05), train loss 0.00037\n","Training for epoch 300 done, starting evaluation\n","Epoch 300/350 (lr=1e-05), train loss 0.00038, valid loss 0.00022\n","Model performance:\n","  metrics/test.rmse:           1.22\n","  metrics/test.rmse_pcutoff:   1.22\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 301/350 (lr=1e-05), train loss 0.00048\n","Epoch 302/350 (lr=1e-05), train loss 0.00032\n","Epoch 303/350 (lr=1e-05), train loss 0.00036\n","Epoch 304/350 (lr=1e-05), train loss 0.00034\n","Epoch 305/350 (lr=1e-05), train loss 0.00035\n","Epoch 306/350 (lr=1e-05), train loss 0.00034\n","Epoch 307/350 (lr=1e-05), train loss 0.00042\n","Epoch 308/350 (lr=1e-05), train loss 0.00037\n","Epoch 309/350 (lr=1e-05), train loss 0.00043\n","Training for epoch 310 done, starting evaluation\n","Epoch 310/350 (lr=1e-05), train loss 0.00034, valid loss 0.00023\n","Model performance:\n","  metrics/test.rmse:           1.21\n","  metrics/test.rmse_pcutoff:   1.21\n","  metrics/test.mAP:          100.00\n","  metrics/test.mAR:          100.00\n","Epoch 311/350 (lr=1e-05), train loss 0.00032\n","Epoch 312/350 (lr=1e-05), train loss 0.00044\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3835784273.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m deeplabcut.train_network(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/compat.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, epochs, save_epochs, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix, superanimal_name, superanimal_transfer_learning, engine, device, snapshot_path, detector_path, batch_size, detector_batch_size, detector_epochs, detector_save_epochs, pose_threshold, pytorch_cfg_updates)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_estimation_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 287\u001b[0;31m         return train_network(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/apis/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, modelprefix, device, snapshot_path, detector_path, load_head_weights, batch_size, epochs, save_epochs, detector_batch_size, detector_epochs, detector_save_epochs, display_iters, max_snapshots_to_keep, pose_threshold, pytorch_cfg_updates)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_settings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 374\u001b[0;31m         train(\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mrun_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/apis/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, run_config, task, device, gpus, logger_config, snapshot_path, transform, inference_transform, max_snapshots_to_keep, load_head_weights)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting pose model training...\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 198\u001b[0;31m     runner.fit(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/runners/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, epochs, display_iters)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 214\u001b[0;31m             train_loss = self._epoch(\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/runners/train.py\u001b[0m in \u001b[0;36m_epoch\u001b[0;34m(self, loader, mode, display_iters)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mloss_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 275\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mlosses_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"total_loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_based_on_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 192\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0moriginal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/data/image.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(filepath, color_mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m---\u003e 35\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# This code is meant forcontinuing training from a previous snapshot\n","\n","\n","deeplabcut.train_network(\n","    config,\n","    shuffle=shuffle,\n","    save_epochs=5, #saves an instance of your model every 5 epochs\n","    epochs=200, #tells you how many epochs (rounds of viewing the whole train dataset) the training session will run for\n","    batch_size=8,\n","    snapshot_path=\"/content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/dlc-models-pytorch/iteration-0/openfieldOct30-trainset95shuffle1001/train/snapshot-150.pt\" #replace with your snapshot path\n",")\n"]},{"cell_type":"markdown","id":"42erKswOLyou","metadata":{"id":"42erKswOLyou"},"source":["Note, that **if you hit \"STOP\" you will get a `KeyboardInterrupt` \"error\"! No worries! :)**"]},{"cell_type":"markdown","id":"LOnb4sSFL6dl","metadata":{"id":"LOnb4sSFL6dl"},"source":["## Start evaluating:\n","This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n","and stores the results as .csv file in a subdirectory under **evaluation-results-pytorch**"]},{"cell_type":"code","execution_count":8,"id":"d3f784b5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26975,"status":"ok","timestamp":1758050196183,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"d3f784b5","outputId":"9409a8fc-edf8-40fa-e6fe-19f6632ae801"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluation scorer: DLC_Resnet50_openfieldOct30shuffle1001_snapshot_best-160\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 110/110 [00:03\u003c00:00, 33.97it/s]\n","100%|██████████| 6/6 [00:00\u003c00:00, 43.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluation results file: DLC_Resnet50_openfieldOct30shuffle1001_snapshot_best-160-results.csv\n","Evaluation results for DLC_Resnet50_openfieldOct30shuffle1001_snapshot_best-160-results.csv (pcutoff: 0.4):\n","train rmse              1.16\n","train rmse_pcutoff      1.16\n","train mAP              99.50\n","train mAR              99.55\n","test rmse               1.20\n","test rmse_pcutoff       1.20\n","test mAP              100.00\n","test mAR              100.00\n","Name: (0.95, 1001, 160, -1, 0.4), dtype: float64\n"]}],"source":["# Evaluate the trained model\n","deeplabcut.evaluate_network(config, Shuffles=[shuffle], plotting=True)"]},{"cell_type":"markdown","id":"WshxvQ5qL_YO","metadata":{"id":"WshxvQ5qL_YO"},"source":["Here you want to see a low pixel error (RMSE)! Of course, it can only be as good as the labeler, so be sure your labels are good!\n","\n","RMSE is the average Euclidean error (in pixels) between the predicted keypoint locations and the ground truth labels on a test set.\n","Lower RMSE = better accuracy.\n","For example, an RMSE of 2 px means, on average, predictions are off by 2 pixels from the true label.\n","\n","mAP uses precision and recall\n","- Precision measures how many of the predicted keypoints are correct (i.e., how accurate your predictions are). (within a given cut-off)\n","- Recall measures how many of the actual keypoints were successfully detected.\n","\n","Average precision (AP) is essentially the area under the precision-recall curve for that specific keypoint.\n","After computing AP for each keypoint, you take the mean across all keypoints\n","mAP ranges from 0 → 1 (or sometimes reported as 0–100%).\n","Higher mAP = better.\n","Example: mAP = 0.95 means the network finds the correct location with high confidence most of the time."]},{"cell_type":"code","execution_count":5,"id":"3J3IzyAINf6q","metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1758054450764,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"3J3IzyAINf6q"},"outputs":[],"source":["video1 = \"/content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\"\n"]},{"cell_type":"code","execution_count":18,"id":"a9db7e04","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":327},"executionInfo":{"elapsed":69187,"status":"ok","timestamp":1758051523998,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"a9db7e04","outputId":"40ecba78-30a6-42b3-d173-d2ac0e905beb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Analyzing videos with /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/dlc-models-pytorch/iteration-0/openfieldOct30-trainset95shuffle1001/train/snapshot-best-160.pt\n","Using scorer: DLC_Resnet50_openfieldOct30shuffle1001_snapshot_best-160\n","Starting to analyze /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\n","Video metadata: \n","  Overall # of frames:    2330\n","  Duration of video [s]:  77.67\n","  fps:                    30.0\n","  resolution:             w=640, h=480\n","\n","Running pose prediction with batch size 4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2330/2330 [01:06\u003c00:00, 35.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Saving results in /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4DLC_Resnet50_openfieldOct30shuffle1001_snapshot_best-160.h5 and /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4DLC_Resnet50_openfieldOct30shuffle1001_snapshot_best-160_full.pickle\n","The videos are analyzed. Now your research can truly start!\n","You can create labeled videos with 'create_labeled_video'.\n","If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'DLC_Resnet50_openfieldOct30shuffle1001_snapshot_best-160'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Apply trained model to videos\n","\n","deeplabcut.analyze_videos(\n","    config,\n","    [video1],\n","    save_as_csv=True,\n","    shuffle=shuffle\n",")"]},{"cell_type":"code","execution_count":19,"id":"424747ed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18531,"status":"ok","timestamp":1758051900125,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"424747ed","outputId":"26fbeaae-50c7-4937-e913-00180073629f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting to process video: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\n","Loading /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4 and data.\n","Duration of video [s]: 77.67, recorded with 30.0 fps!\n","Overall # of frames: 2330 with cropped frame dimensions: 640 480\n","Generating frames and creating video.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2330/2330 [00:16\u003c00:00, 143.87it/s]\n"]},{"data":{"text/plain":["[True]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Create labeled videos\n","deeplabcut.create_labeled_video(config, [video1], shuffle=shuffle)"]},{"cell_type":"code","execution_count":12,"id":"KGnqvPT7NdHg","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24715,"status":"ok","timestamp":1758055625548,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"KGnqvPT7NdHg","outputId":"11bc866a-1bc1-4730-d220-8af518bf436b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting to process video: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\n","Loading /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4 and data.\n","Duration of video [s]: 77.67, recorded with 30.0 fps!\n","Overall # of frames: 2330 with cropped frame dimensions: 640 480\n","Generating frames and creating video.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2330/2330 [00:23\u003c00:00, 97.60it/s]\n"]},{"data":{"text/plain":["[True]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["#The simplest:\n","deeplabcut.create_labeled_video(config, [video1], save_frames=False, shuffle=shuffle)\n"]},{"cell_type":"code","execution_count":17,"id":"5o8OtHGwkcec","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1256,"status":"ok","timestamp":1758051380379,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"5o8OtHGwkcec","outputId":"36872c9e-8756-44cf-dae2-86c87923e8d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting to process video: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\n","Loading /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4 and data.\n","Labeled video already created. Skipping...\n","Starting to process video: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\n","Loading /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4 and data.\n","Labeled video already created. Skipping...\n","Starting to process video: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\n","Loading /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4 and data.\n","Labeled video already created. Skipping...\n","Starting to process video: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4\n","Loading /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run/videos/m3v1mp4.mp4 and data.\n","Labeled video already created. Skipping...\n"]},{"data":{"text/plain":["[None]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Extra options for creating labelled video\n","\n","#Dragged points (trailpoints):\n","deeplabcut.create_labeled_video(config, [video1], videotype=\".mp4\", trailpoints=10, shuffle=shuffle)\n","\n","#Draw skeleton (requires having defined a skeleton in the config.yaml file, i.e. how different key points are connected):\n","deeplabcut.create_labeled_video(config, [video1], videotype=\".mp4\", draw_skeleton=True, shuffle=shuffle)\n"]},{"cell_type":"markdown","id":"TR9IFCefXZ1R","metadata":{"id":"TR9IFCefXZ1R"},"source":["If model needs further refinement, I suggest after creating the videos, to download the entire model folder - open it in the GUI. In principle you should be able to used the analyzed video, but sometimes it can miss. In that case, use the original video, analyze the video anew, and then run extract frames in the GUI. Correct the labels as necessary. Upload the resulting project to Google Colab.\n","Set the config to your new project"]},{"cell_type":"code","execution_count":24,"id":"txaS0HGcYT9i","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3209,"status":"ok","timestamp":1758057623652,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"txaS0HGcYT9i","outputId":"6ef0caa5-6290-4ceb-cd7b-8901bede9d82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"code","execution_count":25,"id":"gcG-r38HXYEx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45609,"status":"ok","timestamp":1758057670646,"user":{"displayName":"Anna Stuckert","userId":"13658774148315874138"},"user_tz":-120},"id":"gcG-r38HXYEx","outputId":"94afb07b-20f4-4297-c4c3-99027e34c7ac"},"outputs":[{"data":{"text/plain":["[(0.95,\n","  1005,\n","  (array([  6, 107, 123,  72,  47,  49, 116,  57,   9,  94, 112,  26,  13,\n","           41, 118,  17,  58,  63, 101, 125,  90, 131,  64,  85, 108, 111,\n","          124, 105,  82, 104,  83,  54,  14,  95,  96, 121,  31, 114,  11,\n","          103,   2,  92,  21,  75,  53,  59,  62, 129,   0,  86,  93,   8,\n","          119,  61, 115,  70,  89,  36,  79,  34,  98,  43,  91,  78,  50,\n","           25,  67,   1,  73,  69,  76,  33,  68,  65,  28,   5,  48,  97,\n","           87,  45, 110,  55, 117, 100, 126,  84,  46,  80,  52, 127,  39,\n","           32,  24,  60,  71, 134, 122,  77, 128,  10, 135, 102,  88,   7,\n","           30, 132, 113,  22,  51,   4,  27,  40, 106,  37,  74, 133,  38,\n","           44,  81,   3,  20,  66,  56, 109,  99,  12,  23,  16,  19]),\n","   array([120,  35,  29, 130,  42,  18,  15])))]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["shuffle = 1005\n","# Define the path to the config file\n","config = \"/content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run-outliersExtracted_TrainingDataCreated/config.yaml\"\n","\n","deeplabcut.create_training_dataset(\n","    config,\n","    net_type=\"resnet_50\",\n","    engine=deeplabcut.Engine.PYTORCH,\n","    Shuffles=[shuffle]  #specify which shuffle index you want - if left empty it will use the next available shuffle\n",")"]},{"cell_type":"code","execution_count":null,"id":"k2YPfMSxZ5HH","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"k2YPfMSxZ5HH"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training with configuration:\n","data:\n","  bbox_margin: 20\n","  colormode: RGB\n","  inference:\n","    normalize_images: True\n","  train:\n","    affine:\n","      p: 0.5\n","      rotation: 30\n","      scaling: [0.5, 1.25]\n","      translation: 0\n","    crop_sampling:\n","      width: 448\n","      height: 448\n","      max_shift: 0.1\n","      method: hybrid\n","    gaussian_noise: 12.75\n","    motion_blur: True\n","    normalize_images: True\n","device: auto\n","metadata:\n","  project_path: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run-outliersExtracted_TrainingDataCreated\n","  pose_config_path: /content/drive/MyDrive/Almería ML workshop/openfield-Pranav-2018-10-30_not_run-outliersExtracted_TrainingDataCreated/dlc-models-pytorch/iteration-1/openfieldOct30-trainset95shuffle1005/train/pytorch_config.yaml\n","  bodyparts: ['snout', 'leftear', 'rightear', 'tailbase']\n","  unique_bodyparts: []\n","  individuals: ['animal']\n","  with_identity: None\n","method: bu\n","model:\n","  backbone:\n","    type: ResNet\n","    model_name: resnet50_gn\n","    output_stride: 16\n","    freeze_bn_stats: False\n","    freeze_bn_weights: False\n","  backbone_output_channels: 2048\n","  heads:\n","    bodypart:\n","      type: HeatmapHead\n","      weight_init: normal\n","      predictor:\n","        type: HeatmapPredictor\n","        apply_sigmoid: False\n","        clip_scores: True\n","        location_refinement: True\n","        locref_std: 7.2801\n","      target_generator:\n","        type: HeatmapGaussianGenerator\n","        num_heatmaps: 4\n","        pos_dist_thresh: 17\n","        heatmap_mode: KEYPOINT\n","        gradient_masking: False\n","        generate_locref: True\n","        locref_std: 7.2801\n","      criterion:\n","        heatmap:\n","          type: WeightedMSECriterion\n","          weight: 1.0\n","        locref:\n","          type: WeightedHuberCriterion\n","          weight: 0.05\n","      heatmap_config:\n","        channels: [2048, 4]\n","        kernel_size: [3]\n","        strides: [2]\n","      locref_config:\n","        channels: [2048, 8]\n","        kernel_size: [3]\n","        strides: [2]\n","net_type: resnet_50\n","runner:\n","  type: PoseTrainingRunner\n","  gpus: None\n","  key_metric: test.mAP\n","  key_metric_asc: True\n","  eval_interval: 10\n","  optimizer:\n","    type: AdamW\n","    params:\n","      lr: 0.0005\n","  scheduler:\n","    type: LRListScheduler\n","    params:\n","      lr_list: [[0.0001], [1e-05]]\n","      milestones: [90, 120]\n","  snapshots:\n","    max_snapshots: 5\n","    save_epochs: 5\n","    save_optimizer_state: False\n","train_settings:\n","  batch_size: 8\n","  dataloader_workers: 0\n","  dataloader_pin_memory: False\n","  display_iters: 500\n","  epochs: 200\n","  seed: 42\n","Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"284e264cf3c64f7c8d5a9c301b8b48fa","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/102M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n","Data Transforms:\n","  Training:   Compose([\n","  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),\n","  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),\n","  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),\n","  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),\n","  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),\n","  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n","], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n","  Validation: Compose([\n","  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n","], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n","Using 129 images and 7 for testing\n","\n","Starting pose model training...\n","--------------------------------------------------\n","Epoch 1/200 (lr=0.0005), train loss 0.01533\n","Epoch 2/200 (lr=0.0005), train loss 0.01133\n","Epoch 3/200 (lr=0.0005), train loss 0.00705\n","Epoch 4/200 (lr=0.0005), train loss 0.00495\n"]},{"ename":"error","evalue":"OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4195351293.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m deeplabcut.train_network(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msave_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#saves an instance of your model every 5 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#tells you how many epochs (rounds of viewing the whole train dataset) the training session will run for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/compat.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, epochs, save_epochs, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix, superanimal_name, superanimal_transfer_learning, engine, device, snapshot_path, detector_path, batch_size, detector_batch_size, detector_epochs, detector_save_epochs, pose_threshold, pytorch_cfg_updates)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_estimation_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 287\u001b[0;31m         return train_network(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/apis/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, modelprefix, device, snapshot_path, detector_path, load_head_weights, batch_size, epochs, save_epochs, detector_batch_size, detector_epochs, detector_save_epochs, display_iters, max_snapshots_to_keep, pose_threshold, pytorch_cfg_updates)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_settings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 374\u001b[0;31m         train(\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mrun_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/apis/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, run_config, task, device, gpus, logger_config, snapshot_path, transform, inference_transform, max_snapshots_to_keep, load_head_weights)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting pose model training...\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 198\u001b[0;31m     runner.fit(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/runners/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, epochs, display_iters)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 214\u001b[0;31m             train_loss = self._epoch(\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/runners/train.py\u001b[0m in \u001b[0;36m_epoch\u001b[0;34m(self, loader, mode, display_iters)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mloss_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 275\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mlosses_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"total_loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_based_on_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 192\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0moriginal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deeplabcut/pose_estimation_pytorch/data/image.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(filepath, color_mode)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 37\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"BGR\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsupported `color_mode`: {color_mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["deeplabcut.train_network(\n","    config,\n","    shuffle=shuffle,\n","    save_epochs=5, #saves an instance of your model every 5 epochs\n","    epochs=200, #tells you how many epochs (rounds of viewing the whole train dataset) the training session will run for\n","    batch_size=8,\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"06a42a901f2046dba0f5e3ad80032f5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2168cefebd8e4f37b413955ab04348ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"284e264cf3c64f7c8d5a9c301b8b48fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7f456bf07d140548dd7c99885b43fcc","IPY_MODEL_4752465ee3b248a892f19f21c1517fb6","IPY_MODEL_8241fa7b6e8d49ea8c06b5491724e0e2"],"layout":"IPY_MODEL_38026f32f9a24dbb8e6b0e8b0ce9b65c"}},"35e0a661743e40cc84417e929d99c316":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38026f32f9a24dbb8e6b0e8b0ce9b65c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384d434726074c538cca34b8db85a62a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4752465ee3b248a892f19f21c1517fb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06a42a901f2046dba0f5e3ad80032f5e","max":102242522,"min":0,"orientation":"horizontal","style":"IPY_MODEL_384d434726074c538cca34b8db85a62a","value":102242522}},"534358a01cf14bd9a44cdbcc2df26421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd364e6ae9664f8988dc700eb5669a8a","max":102242522,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c54d868ffdd64a3eb673909d35aa2ad1","value":102242522}},"5417bf4435374396998fd7aff457d956":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60f1e4b3d0a447f8a7c0c4fe41593b32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7512c9afc8094597bbc748c47fbcc6bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d8acaf8522c46c78c8b4e309ee12ece":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8241fa7b6e8d49ea8c06b5491724e0e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5417bf4435374396998fd7aff457d956","placeholder":"​","style":"IPY_MODEL_35e0a661743e40cc84417e929d99c316","value":" 102M/102M [00:00\u0026lt;00:00, 133MB/s]"}},"b240ed48472a4a42ac5f22fbe84403c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdcb260bf7f645e39f0a4595fade2fcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60f1e4b3d0a447f8a7c0c4fe41593b32","placeholder":"​","style":"IPY_MODEL_7512c9afc8094597bbc748c47fbcc6bc","value":" 102M/102M [00:00\u0026lt;00:00, 195MB/s]"}},"c54d868ffdd64a3eb673909d35aa2ad1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7f456bf07d140548dd7c99885b43fcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d8acaf8522c46c78c8b4e309ee12ece","placeholder":"​","style":"IPY_MODEL_dc5fbac94dde4ae0b10f50d22ade7e9b","value":"model.safetensors: 100%"}},"d00d4fd91ee740628a8cf7c7f23694d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5d23d35dbb740a08ad26d6126499bca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9668738000a449f940278e3426d7d6e","IPY_MODEL_534358a01cf14bd9a44cdbcc2df26421","IPY_MODEL_bdcb260bf7f645e39f0a4595fade2fcc"],"layout":"IPY_MODEL_d00d4fd91ee740628a8cf7c7f23694d0"}},"dc5fbac94dde4ae0b10f50d22ade7e9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd364e6ae9664f8988dc700eb5669a8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9668738000a449f940278e3426d7d6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2168cefebd8e4f37b413955ab04348ae","placeholder":"​","style":"IPY_MODEL_b240ed48472a4a42ac5f22fbe84403c8","value":"model.safetensors: 100%"}}}}},"nbformat":4,"nbformat_minor":5}